{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GPc3QCr9N0kY","ExecuteTime":{"end_time":"2023-10-04T05:24:42.779925Z","start_time":"2023-10-04T05:24:42.763728Z"},"outputId":"16f6a6f7-c14e-49e5-efb3-2537c279e787"},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'feel', 'hungry']\n"]}],"source":["# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n","sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n","\n","# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n","word_list = 'i feel hungry'.split()\n","print(word_list)"]},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"]}],"source":["index_to_word={}  # 빈 딕셔너리를 만들어서\n","\n","# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다.\n","# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다.\n","index_to_word[0]='<PAD>'  # 패딩용 단어\n","index_to_word[1]='<BOS>'  # 문장의 시작지점\n","index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n","index_to_word[3]='i'\n","index_to_word[4]='feel'\n","index_to_word[5]='hungry'\n","index_to_word[6]='eat'\n","index_to_word[7]='lunch'\n","index_to_word[8]='now'\n","index_to_word[9]='happy'\n","\n","print(index_to_word)"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:25:00.166365Z","start_time":"2023-10-04T05:25:00.157735Z"},"id":"1tfsyjjKQCeY","outputId":"d9211339-e420-4d8a-9d84-d7a94385fe1c"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 3, 6, 7]\n"]}],"source":["word_to_index={word:index for index, word in index_to_word.items()}\n","# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다.\n","def get_encoded_sentence(sentence, word_to_index):\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","print(get_encoded_sentence('i eat lunch', word_to_index))"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:25:45.162769Z","start_time":"2023-10-04T05:25:45.155160Z"},"id":"4B4ar5TkQCeZ","outputId":"934e7845-57df-4423-90c5-e74f5fed68d9"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"]}],"source":["# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다.\n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다.\n","encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n","print(encoded_sentences)\n","\n"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:26:27.010252Z","start_time":"2023-10-04T05:26:27.001772Z"},"id":"pQ1FwkIrQCeZ","outputId":"152ffc9c-1354-4640-e97c-b7f4dde4f1fb"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["i feel hungry\n"]}],"source":["# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다.\n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n","\n","print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:26:52.074522Z","start_time":"2023-10-04T05:26:52.066875Z"},"id":"-ODnz4iOQCeZ","outputId":"2da79efd-b3b2-4538-d373-ef589416cddd"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["['i feel hungry', 'i eat lunch', 'now i feel happy']\n"]}],"source":["# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n","def get_decoded_sentences(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n","\n","# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n","print(get_decoded_sentences(encoded_sentences, index_to_word))"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:26:59.442151Z","start_time":"2023-10-04T05:26:59.433221Z"},"id":"mTFo-A7dQCea","outputId":"4d412df6-9947-42cb-d7a0-3c932cb35142"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[[ 0.00902807  0.01218699  0.02877513 -0.00590857]\n","  [ 0.008716    0.00645512  0.03060907 -0.02177706]\n","  [-0.04019227 -0.016194    0.01283761  0.00439799]\n","  [ 0.01281712  0.04282843 -0.02160218  0.01032275]\n","  [-0.03585175  0.00157564 -0.01393794 -0.00278502]]\n","\n"," [[ 0.00902807  0.01218699  0.02877513 -0.00590857]\n","  [ 0.008716    0.00645512  0.03060907 -0.02177706]\n","  [ 0.04285573 -0.01236903 -0.00312948  0.02558334]\n","  [ 0.01372652  0.00030916  0.01791651 -0.04300101]\n","  [-0.03585175  0.00157564 -0.01393794 -0.00278502]]\n","\n"," [[ 0.00902807  0.01218699  0.02877513 -0.00590857]\n","  [-0.00067801  0.03785385  0.02029476  0.01406691]\n","  [ 0.008716    0.00645512  0.03060907 -0.02177706]\n","  [-0.04019227 -0.016194    0.01283761  0.00439799]\n","  [-0.02528045 -0.04720526  0.01606939 -0.03144149]]], shape=(3, 5, 4), dtype=float32)\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import os\n","\n","vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n","word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n","\n","embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n","\n","# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야\n","# embedding 레이어의 input이 될 수 있음에 주의해 주세요.\n","raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n","raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n","                                                       value=word_to_index['<PAD>'],\n","                                                       padding='post',\n","                                                       maxlen=5)\n","output = embedding(raw_inputs)\n","print(output)"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:29:08.580589Z","start_time":"2023-10-04T05:29:08.563715Z"},"id":"tcduJ7MHQCea","outputId":"77488ede-3d54-469b-d43c-c73c5bfceb14"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, None, 4)           40        \n","                                                                 \n"," lstm (LSTM)                 (None, 8)                 416       \n","                                                                 \n"," dense (Dense)               (None, 8)                 72        \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 9         \n","                                                                 \n","=================================================================\n","Total params: 537 (2.10 KB)\n","Trainable params: 537 (2.10 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# RNN 모델 생성\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n","model.add(tf.keras.layers.Dense(8, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","\n","model.summary()"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:36:28.914750Z","start_time":"2023-10-04T05:36:28.683377Z"},"id":"IbHVweS6QCea","outputId":"458e4804-7539-4b00-e23f-39c8d4035b4a"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_4 (Embedding)     (None, None, 4)           40        \n","                                                                 \n"," conv1d (Conv1D)             (None, None, 16)          464       \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, None, 16)          0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, None, 16)          1808      \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 16)                0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense_2 (Dense)             (None, 8)                 136       \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 9         \n","                                                                 \n","=================================================================\n","Total params: 2457 (9.60 KB)\n","Trainable params: 2457 (9.60 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# CNN을 이용한 텍스트 처리\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n","model.add(tf.keras.layers.MaxPooling1D(5))\n","model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n","model.add(tf.keras.layers.GlobalMaxPooling1D())\n","model.add(tf.keras.layers.Dense(8, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","\n","model.summary()"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:58:53.270949Z","start_time":"2023-10-04T05:58:53.192645Z"},"id":"i4sP51yWQCeb","outputId":"3e3cfc76-344e-46bb-b516-03e9c59a53a4"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_5 (Embedding)     (None, None, 4)           40        \n","                                                                 \n"," global_max_pooling1d_1 (Gl  (None, 4)                 0         \n"," obalMaxPooling1D)                                               \n","                                                                 \n"," dense_4 (Dense)             (None, 8)                 40        \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 9         \n","                                                                 \n","=================================================================\n","Total params: 89 (356.00 Byte)\n","Trainable params: 89 (356.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# GlobalMaxPooling1D() 레이어 하나만 사용\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(tf.keras.layers.GlobalMaxPooling1D())\n","model.add(tf.keras.layers.Dense(8, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","\n","model.summary()"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T05:59:07.985039Z","start_time":"2023-10-04T05:59:07.942738Z"},"id":"r3J-kTpGQCeb","outputId":"a170dd00-fe6b-446d-d6c2-a02abc30e446"}},{"cell_type":"markdown","source":["# IMDB 데이터셋 분석"],"metadata":{"collapsed":false,"id":"xnlnIOnAQCeb"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 3s 0us/step\n","훈련 샘플 개수: 25000, 테스트 개수: 25000\n"]}],"source":["imdb = tf.keras.datasets.imdb\n","\n","# IMDb 데이터셋 다운로드\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n","print(f\"훈련 샘플 개수: {len(x_train)}, 테스트 개수: {len(x_test)}\")"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T06:02:02.710111Z","start_time":"2023-10-04T06:01:55.731489Z"},"id":"2GT5jupKQCec","outputId":"a8cc891e-f935-4b17-ba41-dc98cd82a68a"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"]}],"source":["word_to_index = imdb.get_word_index()\n","index_to_word = {index:word for word, index in word_to_index.items()}\n","\n","# 보정 전 x_train[0] 데이터\n","print(get_decoded_sentence(x_train[0], index_to_word))"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T06:06:20.924020Z","start_time":"2023-10-04T06:06:20.857369Z"},"id":"mmF-_fEFQCec","outputId":"724b36bc-7461-41df-f964-639f1c3a76ee"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["<BOS>\n","4\n","the\n","this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"]}],"source":["#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.\n","word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n","\n","# 처음 몇 개 인덱스는 사전에 정의되어 있습니다.\n","word_to_index[\"<PAD>\"] = 0\n","word_to_index[\"<BOS>\"] = 1\n","word_to_index[\"<UNK>\"] = 2  # unknown\n","word_to_index[\"<UNUSED>\"] = 3\n","\n","index_to_word = {index:word for word, index in word_to_index.items()}\n","\n","print(index_to_word[1])     # '<BOS>' 가 출력됩니다.\n","print(word_to_index['the'])  # 4 이 출력됩니다.\n","print(index_to_word[4])     # 'the' 가 출력됩니다.\n","\n","# 보정 후 x_train[0] 데이터\n","print(get_decoded_sentence(x_train[0], index_to_word))"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T06:06:31.756092Z","start_time":"2023-10-04T06:06:31.746917Z"},"id":"yUoPQu8eQCec","outputId":"b46a8b84-81e8-41fd-9539-39e6e3435380"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["문장길이 평균 :  234.75892\n","문장길이 최대 :  2494\n","문장길이 표준편차 :  172.91149458735703\n","pad_sequences maxlen :  580\n","전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"]}],"source":["total_data_text = list(x_train) + list(x_test)\n","# 텍스트데이터 문장길이의 리스트를 생성한 후\n","num_tokens = [len(tokens) for tokens in total_data_text]\n","num_tokens = np.array(num_tokens)\n","# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다.\n","print('문장길이 평균 : ', np.mean(num_tokens))\n","print('문장길이 최대 : ', np.max(num_tokens))\n","print('문장길이 표준편차 : ', np.std(num_tokens))\n","\n","# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,\n","max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","maxlen = int(max_tokens)\n","print('pad_sequences maxlen : ', maxlen)\n","print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 maxlen 설정값 이내에 포함됩니다. ')"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T06:06:55.276268Z","start_time":"2023-10-04T06:06:55.246349Z"},"id":"dXfzUkE4QCec","outputId":"f771490e-3cf2-4104-f69a-e27219bea6de"}},{"cell_type":"code","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(25000, 580)\n"]}],"source":["x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n","                                                        value=word_to_index[\"<PAD>\"],\n","                                                        padding='post', # 혹은 'pre'\n","                                                        maxlen=maxlen)\n","\n","x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n","                                                       value=word_to_index[\"<PAD>\"],\n","                                                       padding='post', # 혹은 'pre'\n","                                                       maxlen=maxlen)\n","\n","print(x_train.shape)"],"metadata":{"ExecuteTime":{"end_time":"2023-10-04T06:07:18.546328Z","start_time":"2023-10-04T06:07:17.843439Z"},"id":"-BSXpKnpQCed","outputId":"b370499c-230b-4aca-c3a4-e11d99d668bc"}}]}