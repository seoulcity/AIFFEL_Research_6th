{"cells":[{"cell_type":"markdown","metadata":{"id":"VDzGB5QX26H3"},"source":["# 11-3 집합과 시퀀스"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8435,"status":"ok","timestamp":1698736310560,"user":{"displayName":"­김정현","userId":"07015025296255556159"},"user_tz":-540},"id":"4p4WSKEo21Qt","outputId":"20647421-efa2-430b-8fd3-f50109e69c07"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  61.4M      0  0:00:01  0:00:01 --:--:-- 61.5M\n"]}],"source":["# IMDB 영화 리뷰 데이터 준비\n","\n","!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WcsvpXI3N8e"},"outputs":[],"source":["!rm -r aclImdb/train/unsup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFN-ZpQz3r8Z","executionInfo":{"status":"ok","timestamp":1698736310561,"user_tz":-540,"elapsed":4,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9222e8a3-f55e-4ced-98ef-bbb52b41b4c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"]}],"source":["!cat aclImdb/train/pos/4077_10.txt"]},{"cell_type":"code","source":["import os, pathlib, shutil, random\n","\n","base_dir = pathlib.Path('aclImdb')\n","val_dir = base_dir / 'val'\n","train_dir = base_dir / 'train'\n","for category in ('neg', 'pos'):\n","  os.makedirs(val_dir / category)\n","  files = os.listdir(train_dir / category)\n","  random.Random(1337).shuffle(files)\n","  num_val_samples = int(0.2 * len(files))\n","  val_files = files[-num_val_samples:]\n","  for fname in val_files:\n","    shutil.move(train_dir / category / fname,\n","                val_dir / category / fname)"],"metadata":{"id":"e7BUieO3jcn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","batch_size = 32\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    'aclImdb/train',\n","    batch_size = batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    'aclImdb/val',\n","    batch_size = batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    'aclImdb/test',\n","    batch_size = batch_size\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiCOwNwmkwUZ","executionInfo":{"status":"ok","timestamp":1698736409931,"user_tz":-540,"elapsed":11241,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"165a5ed3-2d1e-41d6-9077-8357f3039805"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["for inputs, targets in train_ds:\n","  print('inputs.shape:', inputs.shape)\n","  print('inputs.dtype:', inputs.dtype)\n","  print('targets.shape:', targets.shape)\n","  print('targets.dtype:', targets.dtype)\n","  print('inputs[0]:', inputs[0])\n","  print('targets[0]:', targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Enx6KnTtlHYk","executionInfo":{"status":"ok","timestamp":1698736550035,"user_tz":-540,"elapsed":3,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"f3a11071-d29f-4c3e-a827-52818ece060b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32,)\n","inputs.dtype: <dtype: 'string'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor(b\"Dan Katzir has produced a wonderful film that takes us on a roller-coaster ride through a real romance set in the troubles surrounding modern Israel.<br /><br />For anyone who's ever been in love, the film brings back the uncertainties, the insecurities and heartache that make love so bitter-sweet. The atmosphere of fear and isolation that came with the difficult times in Israel at that time just serve to intensify the feeling. Instantly, you are drawn in to Dan's plight, and you can't fail to be deeply moved.<br /><br />You can't write drama and passion like this - the contrast between the realities of Dan's desperate, snatched relationship with Iris, and the realities of a state in turmoil make this eminently watchable. If you have an ounce of passion, and have ever been in love, see this film.\", shape=(), dtype=string)\n","targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["from keras.layers import TextVectorization\n","# One-hot 인코딩 방식의 BoW 방식\n","text_vectorization = TextVectorization(\n","    max_tokens=20000,\n","    output_mode='multi_hot',\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(text_only_train_ds)\n","binary_1gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n"],"metadata":{"id":"L570rmjOmc3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 변환 데이터 확인\n","for inputs, targets in binary_1gram_train_ds:\n","  print('inputs.shape:', inputs.shape)\n","  print('inputs.dtype:', inputs.dtype)\n","  print('targets.shape:', targets.shape)\n","  print('targets.dtype:', targets.dtype)\n","  print('inputs[0]:', inputs[0])\n","  print('targets[0]:', targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxhghKY0nvnL","executionInfo":{"status":"ok","timestamp":1698737239562,"user_tz":-540,"elapsed":286,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"d76900db-01b4-4d16-d5fb-d727a6ec493f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["# 모델 생성\n","from tensorflow import keras\n","from keras import layers\n","\n","def get_model(max_tokens=20000, hidden_dim=16):\n","  inputs = keras.Input(shape=(max_tokens,))\n","  x = layers.Dense(hidden_dim, activation='relu')(inputs)\n","  x = layers.Dropout(0.5)(x)\n","  outputs = layers.Dense(1, activation='sigmoid')(x)\n","  model = keras.Model(inputs, outputs)\n","  model.compile(optimizer='rmsprop',\n","                loss='binary_crossentropy',\n","                metrics=['accuracy'])\n","  return model\n","\n","model = get_model()\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GaXdPpAkoapz","executionInfo":{"status":"ok","timestamp":1698737529391,"user_tz":-540,"elapsed":336,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"ebb1dd2d-cb9c-4896-c486-71a28bb20080"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense (Dense)               (None, 16)                320016    \n","                                                                 \n"," dropout (Dropout)           (None, 16)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320033 (1.22 MB)\n","Trainable params: 320033 (1.22 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint('binary_1gram_keras',\n","                                    save_best_only=True)\n","]\n","model.fit(binary_1gram_train_ds.cache(),\n","          validation_data=binary_1gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model('binary_1gram_keras')\n","print(f'테스트 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9do9QtqupW4E","executionInfo":{"status":"ok","timestamp":1698737753549,"user_tz":-540,"elapsed":48786,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"744f246e-965b-425e-f9e3-b07f08f14e21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 12s 10ms/step - loss: 0.4328 - accuracy: 0.8146 - val_loss: 0.3046 - val_accuracy: 0.8720\n","Epoch 2/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.2893 - accuracy: 0.8942 - val_loss: 0.2864 - val_accuracy: 0.8862\n","Epoch 3/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2537 - accuracy: 0.9107 - val_loss: 0.2895 - val_accuracy: 0.8872\n","Epoch 4/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.2356 - accuracy: 0.9190 - val_loss: 0.3082 - val_accuracy: 0.8810\n","Epoch 5/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.2219 - accuracy: 0.9237 - val_loss: 0.3158 - val_accuracy: 0.8824\n","Epoch 6/10\n","625/625 [==============================] - 4s 6ms/step - loss: 0.2206 - accuracy: 0.9273 - val_loss: 0.3264 - val_accuracy: 0.8774\n","Epoch 7/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2070 - accuracy: 0.9314 - val_loss: 0.3369 - val_accuracy: 0.8808\n","Epoch 8/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2164 - accuracy: 0.9311 - val_loss: 0.3492 - val_accuracy: 0.8732\n","Epoch 9/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.2106 - accuracy: 0.9338 - val_loss: 0.3557 - val_accuracy: 0.8794\n","Epoch 10/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.2052 - accuracy: 0.9352 - val_loss: 0.3662 - val_accuracy: 0.8726\n","782/782 [==============================] - 4s 6ms/step - loss: 0.2982 - accuracy: 0.8787\n","테스트 정확도: 0.879\n"]}]},{"cell_type":"code","source":["# 바이그램 반환 텍스트 벡터화 층 만들기\n","text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode='multi_hot',\n",")\n","\n","# 모델 훈련 및 테스트\n","text_vectorization.adapt(text_only_train_ds)\n","binary_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint('binary_2gram_keras',\n","                                    save_best_only=True)\n","]\n","model.fit(binary_2gram_train_ds.cache(),\n","          validation_data=binary_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model('binary_2gram_keras')\n","print(f'테스트 정확도: {model.evaluate(binary_2gram_test_ds)[1]:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5wFe9Xdug1F","executionInfo":{"status":"ok","timestamp":1698739229253,"user_tz":-540,"elapsed":65449,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"ee738e3e-e1f4-488f-871b-ea8110924b70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 8s 11ms/step - loss: 0.3888 - accuracy: 0.8403 - val_loss: 0.2657 - val_accuracy: 0.8970\n","Epoch 2/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.2473 - accuracy: 0.9100 - val_loss: 0.2576 - val_accuracy: 0.9038\n","Epoch 3/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2160 - accuracy: 0.9269 - val_loss: 0.2728 - val_accuracy: 0.9022\n","Epoch 4/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.1901 - accuracy: 0.9375 - val_loss: 0.2870 - val_accuracy: 0.9038\n","Epoch 5/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1830 - accuracy: 0.9419 - val_loss: 0.3044 - val_accuracy: 0.8996\n","Epoch 6/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1727 - accuracy: 0.9481 - val_loss: 0.3323 - val_accuracy: 0.8958\n","Epoch 7/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1630 - accuracy: 0.9503 - val_loss: 0.3364 - val_accuracy: 0.8966\n","Epoch 8/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.1684 - accuracy: 0.9553 - val_loss: 0.3424 - val_accuracy: 0.8962\n","Epoch 9/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1650 - accuracy: 0.9531 - val_loss: 0.3504 - val_accuracy: 0.8968\n","Epoch 10/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1612 - accuracy: 0.9556 - val_loss: 0.3604 - val_accuracy: 0.8988\n","782/782 [==============================] - 6s 7ms/step - loss: 0.2668 - accuracy: 0.9006\n","테스트 정확도: 0.901\n"]}]},{"cell_type":"code","source":["# 단어 빈도-역문서 빈도(Term Frequency-Inverse Document Frequency) 정규화\n","text_vectorization = TextVectorization(ngrams=2, max_tokens=20000, output_mode='tf_idf',)\n","\n","# 모델 훈련 및 테스트\n","text_vectorization.adapt(text_only_train_ds)\n","tfidf_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint('tfidf_2gram_keras',\n","                                    save_best_only=True)\n","]\n","model.fit(tfidf_2gram_train_ds.cache(),\n","          validation_data=tfidf_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model('tfidf_2gram_keras')\n","print(f'테스트 정확도: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}')"],"metadata":{"id":"cNu2apCrvqET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 11.3.3 시퀀스 모델 방식"],"metadata":{"id":"MG5wyHNqv5i4"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","max_length = 600\n","max_tokens = 20000\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode='int',\n","    output_sequence_length=max_length,\n",")\n","text_vectorization.adapt(text_only_train_ds)\n","int_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"],"metadata":{"id":"c9ZOhtSOyLra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","inputs = keras.Input(shape=(None,), dtype='int64')\n","embedded = tf.one_hot(inputs, depth=max_tokens)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation='sigmoid')(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMnsa_1Py7TM","executionInfo":{"status":"ok","timestamp":1698740156548,"user_tz":-540,"elapsed":739,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"7bf18eaa-dc62-40c7-dcdb-e1f5b5a46424"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," tf.one_hot_1 (TFOpLambda)   (None, None, 20000)       0         \n","                                                                 \n"," bidirectional_1 (Bidirecti  (None, 64)                5128448   \n"," onal)                                                           \n","                                                                 \n"," dropout_3 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5128513 (19.56 MB)\n","Trainable params: 5128513 (19.56 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint('one_hot_bidir_lstm_keras', save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model('one_hot_bidir_lstm_keras')\n","print(f'테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":758},"id":"7BbABUJCzeiS","executionInfo":{"status":"error","timestamp":1698742261645,"user_tz":-540,"elapsed":1973569,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"085be71d-d0af-421e-8e89-e002706129bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 164s 253ms/step - loss: 0.5538 - accuracy: 0.7181 - val_loss: 0.3529 - val_accuracy: 0.8688\n","Epoch 2/10\n","625/625 [==============================] - 164s 262ms/step - loss: 0.3663 - accuracy: 0.8643 - val_loss: 0.2982 - val_accuracy: 0.8860\n","Epoch 3/10\n","625/625 [==============================] - 155s 247ms/step - loss: 0.2986 - accuracy: 0.8949 - val_loss: 0.5752 - val_accuracy: 0.8274\n","Epoch 4/10\n","625/625 [==============================] - 155s 249ms/step - loss: 0.2531 - accuracy: 0.9136 - val_loss: 0.3037 - val_accuracy: 0.8842\n","Epoch 5/10\n","625/625 [==============================] - 165s 264ms/step - loss: 0.2263 - accuracy: 0.9261 - val_loss: 0.2870 - val_accuracy: 0.8930\n","Epoch 6/10\n","625/625 [==============================] - 152s 242ms/step - loss: 0.1941 - accuracy: 0.9372 - val_loss: 0.2890 - val_accuracy: 0.8908\n","Epoch 7/10\n","625/625 [==============================] - 154s 247ms/step - loss: 0.1711 - accuracy: 0.9442 - val_loss: 0.3308 - val_accuracy: 0.8902\n","Epoch 8/10\n","625/625 [==============================] - 154s 247ms/step - loss: 0.1546 - accuracy: 0.9503 - val_loss: 0.4083 - val_accuracy: 0.8616\n","Epoch 9/10\n","625/625 [==============================] - 154s 246ms/step - loss: 0.1348 - accuracy: 0.9576 - val_loss: 0.3628 - val_accuracy: 0.8780\n","Epoch 10/10\n","625/625 [==============================] - 157s 251ms/step - loss: 0.1127 - accuracy: 0.9660 - val_loss: 0.4731 - val_accuracy: 0.8708\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-15d5115cb045>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n\u001b[1;32m      5\u001b[0m           callbacks=callbacks)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ont_hot_bidir_lstm_keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at ont_hot_bidir_lstm_keras"]}]},{"cell_type":"code","source":["# 단어 임베딩 층을 이용한 모델\n","inputs = keras.Input(shape=(None,), dtype='int64')\n","embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation='sigmoid')(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","callbacks = [keras.callbacks.ModelCheckpoint('embeddings_bidir_lstm_keras',\n","                                             save_best_only=True)]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model('embeddings_bidir_lstm_keras')\n","print(f'테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}')"],"metadata":{"id":"RSo4X0860ciR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 11.4.3 트랜스포머 인코더로 영화 리뷰 감성 분류"],"metadata":{"id":"bVedTdVhGmYe"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncode(layers.Layer):\n","  def __init__(self, embed_dim, desne_dim, num_heads, **kwargs):\n","    super().__init__(**kwargs)\n","    self.embed_dim = embed_dim\n","    self.dense_dim = dense_dim\n","    self.num_heads = num_heads\n","    self.attention = layers.MultiheadAttention(\n","        num_heads=num_heads, key_dim=embed_dim)\n","    self.dense_proj = keras.Sequential(\n","        [layers.Dense(dense_dim, activation='relu'),\n","         layers.Dense(embed_dim),]\n","    )\n","    self.layernorm_1 = layers.LayerNormalization()\n","    self.layernorm_2 = layers.LayerNormalization()\n","\n","  def call(self, inputs, mask=None):\n","    if mask is not None:\n","      mask = mask[:, tf.newaxis, :]\n","    attention_output = self.attention(\n","        inputs, inputs, attention_mask=mask)\n","    proj_input = self.layernorm_1(inputs + attention_output)\n","    proj_output = self.dense_proj(proj_input)\n","    return self.layernorm_2(porj_input + proj_output)\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","        'embed_dim': self.embed_dim,\n","        'num_heads': self.num_heads,\n","        'dense_dim': self.dense_dim,\n","    })\n","    return config"],"metadata":{"id":"6vVKHz6h3KW3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##11.5.3 트랜스포머 seq2seq"],"metadata":{"id":"9o7wM1UTknbP"}},{"cell_type":"code","source":["!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","!unzip -q spa-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8h1ryzxzzOT","executionInfo":{"status":"ok","timestamp":1698891276345,"user_tz":-540,"elapsed":792,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"4abb1a70-0897-467a-bf7d-0d30a953c563"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-02 02:14:35--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.207, 142.250.101.207, 142.250.141.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.207|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2638744 (2.5M) [application/zip]\n","Saving to: ‘spa-eng.zip’\n","\n","\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.009s  \n","\n","2023-11-02 02:14:35 (290 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n","\n"]}]},{"cell_type":"code","source":["text_file = \"spa-eng/spa.txt\"\n","with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    english, spanish = line.split(\"\\t\")\n","    spanish = \"[start] \" + spanish + \" [end]\"\n","    text_pairs.append((english, spanish))"],"metadata":{"id":"Aqcd1BU0z6OD","executionInfo":{"status":"ok","timestamp":1698891284570,"user_tz":-540,"elapsed":365,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import random\n","print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXI7uiwDz-nD","executionInfo":{"status":"ok","timestamp":1698891297438,"user_tz":-540,"elapsed":349,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"87114a4f-dbcf-4137-cd7d-ffa70dd3cb49"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["('And what happened after that?', '[start] ¿Y qué pasó después? [end]')\n"]}]},{"cell_type":"code","source":["import random\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]"],"metadata":{"id":"-A5VDfLFz_bt","executionInfo":{"status":"ok","timestamp":1698891311705,"user_tz":-540,"elapsed":546,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import string\n","import re\n","\n","strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_spanish_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"],"metadata":{"id":"T5edSbkr0EY5","executionInfo":{"status":"ok","timestamp":1698891350475,"user_tz":-540,"elapsed":20118,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","\n","def format_dataset(eng, spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\n","        \"english\": eng,\n","        \"spanish\": spa[:, :-1],\n","    }, spa[:, 1:])\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSc6WiCB0HeG","executionInfo":{"status":"ok","timestamp":1698891351609,"user_tz":-540,"elapsed":1145,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"c6d9e840-8686-4649-af68-f4b6c0a596e4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (64, 20)\n","inputs['spanish'].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["# 인코더"],"metadata":{"id":"GgDvu22f09hT"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","    super().__init__(**kwargs)\n","    self.embed_dim = embed_dim\n","    self.dense_dim = dense_dim\n","    self.num_heads = num_heads\n","    self.attention = layers.MultiHeadAttention(\n","        num_heads=num_heads, key_dim=embed_dim\n","    )\n","    self.dense_proj = keras.Sequential(\n","        [layers.Dense(dense_dim, activation='relu'),\n","         layers.Dense(embed_dim),]\n","    )\n","    self.layernorm_1 = layers.LayerNormalization()\n","    self.layernorm_2 = layers.LayerNormalization()\n","\n","  def call(self, inputs, mask=None):\n","    if mask is not None:\n","      mask = mask[:, tf.newaxis, :]\n","    attention_output = self.attention(\n","        inputs, inputs, attention_mask=mask)\n","    proj_input = self.layernorm_1(inputs + attention_output)\n","    proj_output = self.dense_proj(proj_input)\n","    return self.layernorm_2(proj_input + proj_output)\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","        'embed_dim': self.embed_dim,\n","        'num_heads': self.num_heads,\n","        'dense_dim': self.dense_dim,\n","    })\n","    return config"],"metadata":{"id":"JoNnBVMV09Un","executionInfo":{"status":"ok","timestamp":1698892946749,"user_tz":-540,"elapsed":404,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# 위치 임베딩"],"metadata":{"id":"qVEBesHf3UQe"}},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","    super().__init__(**kwargs)\n","    self.token_embeddings = layers.Embedding(\n","        input_dim=input_dim, output_dim=output_dim)\n","    self.position_embeddings = layers.Embedding(\n","        input_dim=sequence_length, output_dim=output_dim)\n","    self.sequence_length = sequence_length\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","\n","  def call(self, inputs):\n","    length = tf.shape(inputs)[-1]\n","    positions = tf.range(start=0, limit=length, delta=1)\n","    embedded_tokens = self.token_embeddings(inputs)\n","    embedded_positions = self.position_embeddings(positions)\n","    return embedded_tokens + embedded_positions\n"],"metadata":{"id":"11Y_uFPk3UAL","executionInfo":{"status":"ok","timestamp":1698892912880,"user_tz":-540,"elapsed":382,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# 디코더"],"metadata":{"id":"Z55tyDfU0_Jz"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","class TransformerDecoder(layers.Layer):\n","  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","    super().__init__(**kwargs)\n","    self.embed_dim = embed_dim\n","    self.dense_dim = dense_dim\n","    self.num_heads = num_heads\n","    self.attention_1 = layers.MultiHeadAttention(\n","        num_heads=num_heads,\n","        key_dim=embed_dim\n","    )\n","    self.attention_2 = layers.MultiHeadAttention(\n","        num_heads=num_heads,\n","        key_dim=embed_dim\n","    )\n","    self.dense_proj = keras.Sequential(\n","        [layers.Dense(dense_dim, activation='relu'),\n","         layers.Dense(embed_dim),]\n","    )\n","    self.layernorm_1 = layers.LayerNormalization()\n","    self.layernorm_2 = layers.LayerNormalization()\n","    self.layernorm_3 = layers.LayerNormalization()\n","    self.supports_masking = True\n","\n","  def get_config(self):\n","    config = super.get_config()\n","    config.update({\n","        'embed_dim': self.embed_dim,\n","        'num_heads': self.num_heads,\n","        'dense_dim': self.dense_dim,\n","    })\n","    return config\n","\n","  # 커절 마스킹\n","  def get_casual_attention_mask(self, inputs):\n","    input_shape = tf.shape(inputs)\n","    batch_size, sequence_length = input_shape[0], input_shape[1]\n","    i = tf.range(sequence_length)[:, tf.newaxis]\n","    j = tf.range(sequence_length)\n","    mask = tf.cast(i >= j, dtype='int32')\n","    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","    mult = tf.concat(\n","        [tf.expand_dims(batch_size, -1),\n","        tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","    return tf.tile(mask, mult)\n","\n","  def call(self, inputs, encoder_outputs, mask=None):\n","    causal_mask = self.get_casual_attention_mask(inputs)\n","    if mask is not None:\n","      padding_mask = tf.cast(\n","          mask[:, tf.newaxis, :], dtype='int32')\n","      padding_mask = tf.minimum(padding_mask, causal_mask)\n","    attention_output_1 = self.attention_1(\n","        query=inputs,\n","        value=inputs,\n","        key=inputs,\n","        attention_mask=causal_mask)\n","    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","    attention_output_2 = self.attention_2(\n","        query=inputs,\n","        value=inputs,\n","        key=inputs,\n","        attention_mask=causal_mask,)\n","    attention_output_2 = self.layernorm_2(\n","        attention_output_1 + attention_output_2)\n","    proj_output = self.dense_proj(attention_output_2)\n","    return self.layernorm_3(attention_output_2 + proj_output)"],"metadata":{"id":"sDfnj6UMkrhu","executionInfo":{"status":"ok","timestamp":1698893262370,"user_tz":-540,"elapsed":289,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["vocab_size = 20000\n","sequence_length = 600\n","embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='english')\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='spanish')\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x)\n","\n","decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"metadata":{"id":"2-sohatHpdqW","executionInfo":{"status":"ok","timestamp":1698893657308,"user_tz":-540,"elapsed":834,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["transformer.compile(\n","    optimizer='rmsprop',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy'])\n","transformer.fit(train_ds, epochs=30, validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vk93gXt1rKyN","executionInfo":{"status":"error","timestamp":1698893659195,"user_tz":-540,"elapsed":288,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"59764d5e-efd0-431c-e692-11fb216e34d3"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-f13d1995535c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     metrics=['accuracy'])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_filescbnb018.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mproj_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mproj_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filescbnb018.py\", line 26, in tf__call\n        attention_output = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(inputs), ag__.ld(inputs)), dict(attention_mask=ag__.ld(mask)), fscope)\n\n    ValueError: Exception encountered when calling layer 'transformer_encoder_13' (type TransformerEncoder).\n    \n    in user code:\n    \n        File \"<ipython-input-16-554adf28f8c2>\", line 24, in call  *\n            attention_output = self.attention(\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'softmax' (type Softmax).\n        \n        Dimensions must be equal, but are 20 and 256 for '{{node model_5/transformer_encoder_13/multi_head_attention_13/softmax/add}} = AddV2[T=DT_FLOAT](model_5/transformer_encoder_13/multi_head_attention_13/einsum/Einsum, model_5/transformer_encoder_13/multi_head_attention_13/softmax/mul)' with input shapes: [?,8,20,20], [?,1,20,256].\n        \n        Call arguments received by layer 'softmax' (type Softmax):\n          • inputs=tf.Tensor(shape=(None, 8, 20, 20), dtype=float32)\n          • mask=tf.Tensor(shape=(None, 1, 20, 256), dtype=float32)\n    \n    \n    Call arguments received by layer 'transformer_encoder_13' (type TransformerEncoder):\n      • inputs=tf.Tensor(shape=(None, 20, 256), dtype=float32)\n      • mask=tf.Tensor(shape=(None, 20, 256), dtype=float32)\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}